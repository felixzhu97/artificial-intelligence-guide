#!/usr/bin/env python3
"""
ç¬¬24ç« ï¼šæ·±åº¦è‡ªç„¶è¯­è¨€å¤„ç† (Deep Natural Language Processing)

æœ¬æ¨¡å—å®ç°äº†æ·±åº¦å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åº”ç”¨ï¼š
- è¯å‘é‡ (Word Embeddings)
- åºåˆ—åˆ°åºåˆ—æ¨¡å‹ (Seq2Seq)
- æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanism)
- Transformer æ¶æ„
- BERT ç±»æ¨¡å‹åŸºç¡€
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
import re
from collections import Counter, defaultdict
import json

plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

class WordEmbedding:
    """è¯å‘é‡æ¨¡å‹"""
    
    def __init__(self, embedding_dim: int = 100):
        self.embedding_dim = embedding_dim
        self.word_to_idx = {}
        self.idx_to_word = {}
        self.embeddings = None
        self.vocab_size = 0
    
    def build_vocab(self, corpus: List[str], min_count: int = 1):
        """æ„å»ºè¯æ±‡è¡¨"""
        word_counts = Counter()
        
        for sentence in corpus:
            words = self._tokenize(sentence)
            word_counts.update(words)
        
        # è¿‡æ»¤ä½é¢‘è¯
        vocab = [word for word, count in word_counts.items() if count >= min_count]
        
        # æ·»åŠ ç‰¹æ®Šæ ‡è®°
        vocab = ['<PAD>', '<UNK>', '<SOS>', '<EOS>'] + vocab
        
        self.word_to_idx = {word: idx for idx, word in enumerate(vocab)}
        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}
        self.vocab_size = len(vocab)
        
        # åˆå§‹åŒ–éšæœºè¯å‘é‡
        self.embeddings = np.random.normal(0, 0.1, (self.vocab_size, self.embedding_dim))
    
    def _tokenize(self, text: str) -> List[str]:
        """ç®€å•åˆ†è¯"""
        # å¤„ç†ä¸­æ–‡å’Œè‹±æ–‡
        text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text.lower())
        # ä¸­æ–‡å­—ç¬¦åˆ†å‰²
        words = []
        for char in text:
            if '\u4e00' <= char <= '\u9fff':  # ä¸­æ–‡å­—ç¬¦
                words.append(char)
            elif char.isalnum():
                words.append(char)
        
        # åˆå¹¶è¿ç»­çš„è‹±æ–‡å­—ç¬¦
        result = []
        temp_word = ""
        for word in words:
            if word.isalpha() and not ('\u4e00' <= word <= '\u9fff'):
                temp_word += word
            else:
                if temp_word:
                    result.append(temp_word)
                    temp_word = ""
                if word and not word.isspace():
                    result.append(word)
        if temp_word:
            result.append(temp_word)
        
        return [w for w in result if w.strip()]
    
    def text_to_indices(self, text: str) -> List[int]:
        """æ–‡æœ¬è½¬ç´¢å¼•"""
        words = self._tokenize(text)
        return [self.word_to_idx.get(word, self.word_to_idx['<UNK>']) for word in words]
    
    def indices_to_text(self, indices: List[int]) -> str:
        """ç´¢å¼•è½¬æ–‡æœ¬"""
        words = [self.idx_to_word.get(idx, '<UNK>') for idx in indices]
        return ' '.join(words)
    
    def get_embedding(self, word: str) -> np.ndarray:
        """è·å–è¯å‘é‡"""
        idx = self.word_to_idx.get(word, self.word_to_idx['<UNK>'])
        return self.embeddings[idx]
    
    def similarity(self, word1: str, word2: str) -> float:
        """è®¡ç®—è¯ç›¸ä¼¼åº¦"""
        emb1 = self.get_embedding(word1)
        emb2 = self.get_embedding(word2)
        
        # ä½™å¼¦ç›¸ä¼¼åº¦
        norm1 = np.linalg.norm(emb1)
        norm2 = np.linalg.norm(emb2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return np.dot(emb1, emb2) / (norm1 * norm2)
    
    def most_similar(self, word: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """æ‰¾åˆ°æœ€ç›¸ä¼¼çš„è¯"""
        if word not in self.word_to_idx:
            return []
        
        target_emb = self.get_embedding(word)
        similarities = []
        
        for w, idx in self.word_to_idx.items():
            if w != word and w not in ['<PAD>', '<UNK>', '<SOS>', '<EOS>']:
                sim = self.similarity(word, w)
                similarities.append((w, sim))
        
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]

class SimpleRNN:
    """ç®€å•RNNå®ç°"""
    
    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        
        # æƒé‡åˆå§‹åŒ–
        self.Wxh = np.random.randn(hidden_dim, input_dim) * 0.01
        self.Whh = np.random.randn(hidden_dim, hidden_dim) * 0.01
        self.Why = np.random.randn(output_dim, hidden_dim) * 0.01
        self.bh = np.zeros((hidden_dim, 1))
        self.by = np.zeros((output_dim, 1))
    
    def forward(self, inputs: List[np.ndarray], initial_hidden: np.ndarray = None) -> Tuple[List[np.ndarray], List[np.ndarray]]:
        """å‰å‘ä¼ æ’­"""
        if initial_hidden is None:
            h = np.zeros((self.hidden_dim, 1))
        else:
            h = initial_hidden
        
        hidden_states = []
        outputs = []
        
        for x in inputs:
            x = x.reshape(-1, 1)
            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)
            y = np.dot(self.Why, h) + self.by
            
            hidden_states.append(h.copy())
            outputs.append(y.copy())
        
        return outputs, hidden_states
    
    def generate(self, seed_idx: int, length: int, temperature: float = 1.0) -> List[int]:
        """ç”Ÿæˆåºåˆ—"""
        h = np.zeros((self.hidden_dim, 1))
        generated = [seed_idx]
        
        for _ in range(length):
            x = np.zeros((self.input_dim, 1))
            x[generated[-1]] = 1
            
            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)
            y = np.dot(self.Why, h) + self.by
            
            # æ¸©åº¦é‡‡æ ·
            p = self._softmax(y.flatten() / temperature)
            next_idx = np.random.choice(len(p), p=p)
            generated.append(next_idx)
        
        return generated[1:]  # æ’é™¤ç§å­
    
    def _softmax(self, x: np.ndarray) -> np.ndarray:
        """Softmaxå‡½æ•°"""
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)

class AttentionMechanism:
    """æ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(self, hidden_dim: int):
        self.hidden_dim = hidden_dim
        self.W_attention = np.random.randn(hidden_dim, hidden_dim) * 0.01
        self.v_attention = np.random.randn(hidden_dim, 1) * 0.01
    
    def compute_attention(self, query: np.ndarray, keys: List[np.ndarray], 
                         values: List[np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:
        """è®¡ç®—æ³¨æ„åŠ›æƒé‡å’Œè¾“å‡º"""
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = []
        for key in keys:
            # ç®€åŒ–çš„æ³¨æ„åŠ›è®¡ç®—
            combined = np.tanh(np.dot(self.W_attention, query) + 
                             np.dot(self.W_attention, key))
            score = np.dot(self.v_attention.T, combined)
            scores.append(score[0, 0])
        
        # Softmaxå½’ä¸€åŒ–
        scores = np.array(scores)
        attention_weights = self._softmax(scores)
        
        # åŠ æƒæ±‚å’Œ
        context = np.zeros_like(values[0])
        for i, (weight, value) in enumerate(zip(attention_weights, values)):
            context += weight * value
        
        return context, attention_weights
    
    def _softmax(self, x: np.ndarray) -> np.ndarray:
        """Softmaxå‡½æ•°"""
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)

class SimpleTransformer:
    """ç®€åŒ–çš„Transformerå®ç°"""
    
    def __init__(self, vocab_size: int, d_model: int = 512, n_heads: int = 8, n_layers: int = 6):
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.n_heads = n_heads
        self.n_layers = n_layers
        
        # è¯åµŒå…¥å’Œä½ç½®ç¼–ç 
        self.word_embeddings = np.random.randn(vocab_size, d_model) * 0.01
        self.position_embeddings = self._create_positional_encoding(1000, d_model)
    
    def _create_positional_encoding(self, max_len: int, d_model: int) -> np.ndarray:
        """åˆ›å»ºä½ç½®ç¼–ç """
        pe = np.zeros((max_len, d_model))
        position = np.arange(0, max_len).reshape(-1, 1)
        
        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
        
        pe[:, 0::2] = np.sin(position * div_term)
        pe[:, 1::2] = np.cos(position * div_term)
        
        return pe
    
    def embed(self, tokens: List[int]) -> np.ndarray:
        """åµŒå…¥å±‚"""
        seq_len = len(tokens)
        
        # è¯åµŒå…¥
        embeddings = np.array([self.word_embeddings[token] for token in tokens])
        
        # ä½ç½®ç¼–ç 
        pos_encodings = self.position_embeddings[:seq_len]
        
        return embeddings + pos_encodings
    
    def self_attention(self, x: np.ndarray) -> np.ndarray:
        """ç®€åŒ–çš„è‡ªæ³¨æ„åŠ›"""
        seq_len, d_model = x.shape
        
        # Q, K, V çŸ©é˜µ (ç®€åŒ–å®ç°)
        Q = K = V = x
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = np.dot(Q, K.T) / np.sqrt(d_model)
        
        # Softmax
        attention_weights = self._softmax_2d(scores)
        
        # åŠ æƒæ±‚å’Œ
        output = np.dot(attention_weights, V)
        
        return output
    
    def _softmax_2d(self, x: np.ndarray) -> np.ndarray:
        """2D Softmax"""
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

class BERTLike:
    """ç±»BERTæ¨¡å‹åŸºç¡€æ¡†æ¶"""
    
    def __init__(self, vocab_size: int, hidden_size: int = 768):
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.embeddings = np.random.randn(vocab_size, hidden_size) * 0.01
        
        # ç‰¹æ®Šæ ‡è®°
        self.CLS_TOKEN = 0
        self.SEP_TOKEN = 1
        self.MASK_TOKEN = 2
        self.PAD_TOKEN = 3
    
    def tokenize_and_mask(self, text: str, mask_prob: float = 0.15) -> Tuple[List[int], List[int]]:
        """åˆ†è¯å¹¶éšæœºé®è”½"""
        # ç®€åŒ–çš„åˆ†è¯
        tokens = text.lower().split()
        token_ids = [hash(token) % (self.vocab_size - 10) + 10 for token in tokens]  # ç®€åŒ–æ˜ å°„
        
        # æ·»åŠ ç‰¹æ®Šæ ‡è®°
        token_ids = [self.CLS_TOKEN] + token_ids + [self.SEP_TOKEN]
        
        # éšæœºé®è”½
        masked_ids = token_ids.copy()
        labels = [-1] * len(token_ids)  # -1è¡¨ç¤ºä¸é¢„æµ‹
        
        for i in range(1, len(token_ids) - 1):  # ä¸é®è”½CLSå’ŒSEP
            if np.random.random() < mask_prob:
                labels[i] = token_ids[i]  # ä¿å­˜åŸå§‹tokenä½œä¸ºæ ‡ç­¾
                
                rand = np.random.random()
                if rand < 0.8:
                    masked_ids[i] = self.MASK_TOKEN
                elif rand < 0.9:
                    masked_ids[i] = np.random.randint(4, self.vocab_size)
                # 10%æ¦‚ç‡ä¿æŒåŸæ ·
        
        return masked_ids, labels
    
    def predict_masked_tokens(self, masked_ids: List[int], labels: List[int]) -> Dict[str, Any]:
        """é¢„æµ‹è¢«é®è”½çš„è¯ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        predictions = {}
        
        for i, (masked_id, label) in enumerate(zip(masked_ids, labels)):
            if label != -1:  # éœ€è¦é¢„æµ‹çš„ä½ç½®
                # ç®€åŒ–çš„é¢„æµ‹é€»è¾‘
                if masked_id == self.MASK_TOKEN:
                    # éšæœºé¢„æµ‹ä¸€ä¸ªè¯
                    pred = np.random.randint(4, self.vocab_size)
                    confidence = 0.3 + np.random.random() * 0.4
                else:
                    pred = masked_id
                    confidence = 0.8 + np.random.random() * 0.2
                
                predictions[i] = {
                    'predicted': pred,
                    'actual': label,
                    'confidence': confidence,
                    'correct': pred == label
                }
        
        return predictions

class TextClassifier:
    """æ–‡æœ¬åˆ†ç±»å™¨"""
    
    def __init__(self, embedding_dim: int = 100, hidden_dim: int = 128):
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.classes = []
        self.word_embeddings = None
        
        # ç®€åŒ–çš„ç¥ç»ç½‘ç»œå±‚
        self.W1 = None
        self.b1 = None
        self.W2 = None
        self.b2 = None
    
    def build_model(self, num_classes: int, vocab_size: int):
        """æ„å»ºæ¨¡å‹"""
        self.classes = list(range(num_classes))
        
        # åˆå§‹åŒ–æƒé‡
        self.word_embeddings = np.random.randn(vocab_size, self.embedding_dim) * 0.01
        self.W1 = np.random.randn(self.hidden_dim, self.embedding_dim) * 0.01
        self.b1 = np.zeros((self.hidden_dim, 1))
        self.W2 = np.random.randn(num_classes, self.hidden_dim) * 0.01
        self.b2 = np.zeros((num_classes, 1))
    
    def encode_text(self, token_ids: List[int]) -> np.ndarray:
        """ç¼–ç æ–‡æœ¬"""
        if not token_ids:
            return np.zeros((self.embedding_dim, 1))
        
        # å¹³å‡è¯å‘é‡
        embeddings = np.array([self.word_embeddings[idx] for idx in token_ids])
        mean_embedding = np.mean(embeddings, axis=0).reshape(-1, 1)
        
        return mean_embedding
    
    def predict(self, token_ids: List[int]) -> Tuple[int, List[float]]:
        """é¢„æµ‹åˆ†ç±»"""
        # ç¼–ç 
        x = self.encode_text(token_ids)
        
        # å‰å‘ä¼ æ’­
        h = np.tanh(np.dot(self.W1, x) + self.b1)
        scores = np.dot(self.W2, h) + self.b2
        
        # Softmax
        exp_scores = np.exp(scores - np.max(scores))
        probs = exp_scores / np.sum(exp_scores)
        
        predicted_class = np.argmax(probs)
        
        return predicted_class, probs.flatten().tolist()

def demo_word_embeddings():
    """æ¼”ç¤ºè¯å‘é‡"""
    print("\n" + "="*50)
    print("è¯å‘é‡æ¼”ç¤º")
    print("="*50)
    
    # åˆ›å»ºç®€å•è¯­æ–™åº“
    corpus = [
        "æˆ‘å–œæ¬¢åƒè‹¹æœ",
        "è‹¹æœå¾ˆç”œå¾ˆå¥½åƒ",
        "æˆ‘ä¸å–œæ¬¢åƒæ©˜å­",
        "æ©˜å­å¤ªé…¸äº†",
        "é¦™è•‰å¾ˆç”œ",
        "æˆ‘å–œæ¬¢é¦™è•‰",
        "æ°´æœéƒ½å¾ˆå¥½åƒ",
        "è‹¹æœå’Œé¦™è•‰éƒ½æ˜¯æ°´æœ"
    ]
    
    print("\nè®­ç»ƒè¯­æ–™:")
    for i, sentence in enumerate(corpus, 1):
        print(f"  {i}. {sentence}")
    
    # è®­ç»ƒè¯å‘é‡
    we = WordEmbedding(embedding_dim=50)
    we.build_vocab(corpus)
    
    print(f"\nè¯æ±‡è¡¨å¤§å°: {we.vocab_size}")
    print(f"è¯æ±‡è¡¨å‰20ä¸ªè¯: {list(we.word_to_idx.keys())[:20]}")
    
    # è®¡ç®—è¯ç›¸ä¼¼åº¦
    print(f"\nè¯ç›¸ä¼¼åº¦:")
    word_pairs = [("è‹¹æœ", "é¦™è•‰"), ("å–œæ¬¢", "å¥½åƒ"), ("ç”œ", "é…¸"), ("æ°´æœ", "è‹¹æœ")]
    
    for word1, word2 in word_pairs:
        if word1 in we.word_to_idx and word2 in we.word_to_idx:
            sim = we.similarity(word1, word2)
            print(f"  {word1} - {word2}: {sim:.3f}")
    
    # å¯»æ‰¾æœ€ç›¸ä¼¼è¯
    test_word = "è‹¹æœ"
    if test_word in we.word_to_idx:
        similar_words = we.most_similar(test_word, top_k=3)
        print(f"\nä¸'{test_word}'æœ€ç›¸ä¼¼çš„è¯:")
        for word, sim in similar_words:
            print(f"  {word}: {sim:.3f}")

def demo_rnn_text_generation():
    """æ¼”ç¤ºRNNæ–‡æœ¬ç”Ÿæˆ"""
    print("\n" + "="*50)
    print("RNNæ–‡æœ¬ç”Ÿæˆæ¼”ç¤º")
    print("="*50)
    
    # ç®€å•å­—ç¬¦çº§RNN
    text = "hello world this is a simple text for rnn training"
    chars = list(set(text))
    char_to_idx = {ch: i for i, ch in enumerate(chars)}
    idx_to_char = {i: ch for ch, i in char_to_idx.items()}
    
    print(f"å­—ç¬¦é›†: {chars}")
    print(f"å­—ç¬¦é›†å¤§å°: {len(chars)}")
    
    # åˆ›å»ºRNN
    rnn = SimpleRNN(input_dim=len(chars), hidden_dim=20, output_dim=len(chars))
    
    # æ¨¡æ‹Ÿç”Ÿæˆ
    print(f"\nç”Ÿæˆæ–‡æœ¬ç¤ºä¾‹ (éšæœºåˆå§‹åŒ–æƒé‡):")
    seed_char = 'h'
    if seed_char in char_to_idx:
        seed_idx = char_to_idx[seed_char]
        generated_indices = rnn.generate(seed_idx, length=10, temperature=1.0)
        generated_text = ''.join([idx_to_char.get(idx, '?') for idx in generated_indices])
        print(f"  ç§å­å­—ç¬¦: '{seed_char}'")
        print(f"  ç”Ÿæˆåºåˆ—: '{generated_text}'")
        print("  (æ³¨: æƒé‡æœªè®­ç»ƒï¼Œç”Ÿæˆç»“æœä¸ºéšæœº)")

def demo_attention_mechanism():
    """æ¼”ç¤ºæ³¨æ„åŠ›æœºåˆ¶"""
    print("\n" + "="*50)
    print("æ³¨æ„åŠ›æœºåˆ¶æ¼”ç¤º")
    print("="*50)
    
    # åˆ›å»ºæ³¨æ„åŠ›æœºåˆ¶
    attention = AttentionMechanism(hidden_dim=10)
    
    # æ¨¡æ‹Ÿç¼–ç å™¨éšçŠ¶æ€
    encoder_states = [
        np.random.randn(10, 1),
        np.random.randn(10, 1),
        np.random.randn(10, 1),
        np.random.randn(10, 1)
    ]
    
    # æ¨¡æ‹Ÿè§£ç å™¨æŸ¥è¯¢
    decoder_query = np.random.randn(10, 1)
    
    # è®¡ç®—æ³¨æ„åŠ›
    context, weights = attention.compute_attention(decoder_query, encoder_states, encoder_states)
    
    print(f"ç¼–ç å™¨çŠ¶æ€æ•°é‡: {len(encoder_states)}")
    print(f"æ³¨æ„åŠ›æƒé‡: {weights}")
    print(f"æƒé‡å’Œ: {np.sum(weights):.3f}")
    print(f"ä¸Šä¸‹æ–‡å‘é‡å½¢çŠ¶: {context.shape}")
    
    # å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
    positions = list(range(len(weights)))
    plt.figure(figsize=(8, 4))
    plt.bar(positions, weights, alpha=0.7)
    plt.xlabel('ç¼–ç å™¨ä½ç½®')
    plt.ylabel('æ³¨æ„åŠ›æƒé‡')
    plt.title('æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ')
    plt.xticks(positions)
    for i, w in enumerate(weights):
        plt.text(i, w + 0.01, f'{w:.2f}', ha='center')
    plt.tight_layout()
    plt.savefig('attention_weights.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("æ³¨æ„åŠ›æƒé‡å›¾å·²ä¿å­˜ä¸º 'attention_weights.png'")

def demo_transformer():
    """æ¼”ç¤ºTransformer"""
    print("\n" + "="*50)
    print("Transformeræ¼”ç¤º")
    print("="*50)
    
    # åˆ›å»ºç®€åŒ–çš„Transformer
    vocab_size = 1000
    transformer = SimpleTransformer(vocab_size=vocab_size, d_model=128, n_heads=4, n_layers=2)
    
    # ç¤ºä¾‹è¾“å…¥
    input_tokens = [10, 25, 67, 89, 156]  # æ¨¡æ‹Ÿtoken IDs
    
    print(f"è¾“å…¥token: {input_tokens}")
    print(f"åºåˆ—é•¿åº¦: {len(input_tokens)}")
    
    # åµŒå…¥
    embeddings = transformer.embed(input_tokens)
    print(f"åµŒå…¥å½¢çŠ¶: {embeddings.shape}")
    
    # è‡ªæ³¨æ„åŠ›
    attention_output = transformer.self_attention(embeddings)
    print(f"è‡ªæ³¨æ„åŠ›è¾“å‡ºå½¢çŠ¶: {attention_output.shape}")
    
    # ä½ç½®ç¼–ç å¯è§†åŒ–
    pos_encoding = transformer.position_embeddings[:20, :10]  # å‰20ä¸ªä½ç½®ï¼Œå‰10ä¸ªç»´åº¦
    
    plt.figure(figsize=(10, 6))
    plt.imshow(pos_encoding.T, cmap='coolwarm', aspect='auto')
    plt.colorbar()
    plt.xlabel('ä½ç½®')
    plt.ylabel('ç»´åº¦')
    plt.title('ä½ç½®ç¼–ç å¯è§†åŒ–')
    plt.tight_layout()
    plt.savefig('positional_encoding.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("ä½ç½®ç¼–ç å›¾å·²ä¿å­˜ä¸º 'positional_encoding.png'")

def demo_bert_like():
    """æ¼”ç¤ºBERTç±»æ¨¡å‹"""
    print("\n" + "="*50)
    print("BERTç±»æ¨¡å‹æ¼”ç¤º")
    print("="*50)
    
    # åˆ›å»ºBERTç±»æ¨¡å‹
    bert = BERTLike(vocab_size=1000, hidden_size=256)
    
    # ç¤ºä¾‹æ–‡æœ¬
    text = "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯"
    print(f"åŸå§‹æ–‡æœ¬: {text}")
    
    # åˆ†è¯å’Œé®è”½
    masked_ids, labels = bert.tokenize_and_mask(text, mask_prob=0.3)
    
    print(f"\nåˆ†è¯ç»“æœ:")
    print(f"Token IDs: {masked_ids}")
    print(f"Labels: {labels}")
    
    # æ˜¾ç¤ºé®è”½ä½ç½®
    masked_positions = [i for i, label in enumerate(labels) if label != -1]
    print(f"\né®è”½ä½ç½®: {masked_positions}")
    
    # é¢„æµ‹é®è”½è¯
    predictions = bert.predict_masked_tokens(masked_ids, labels)
    
    print(f"\né®è”½è¯­è¨€æ¨¡å‹é¢„æµ‹:")
    for pos, pred_info in predictions.items():
        status = "âœ“" if pred_info['correct'] else "âœ—"
        print(f"  ä½ç½® {pos}: é¢„æµ‹={pred_info['predicted']}, "
              f"å®é™…={pred_info['actual']}, "
              f"ç½®ä¿¡åº¦={pred_info['confidence']:.2f} {status}")

def demo_text_classification():
    """æ¼”ç¤ºæ–‡æœ¬åˆ†ç±»"""
    print("\n" + "="*50)
    print("æ–‡æœ¬åˆ†ç±»æ¼”ç¤º")
    print("="*50)
    
    # åˆ›å»ºåˆ†ç±»å™¨
    classifier = TextClassifier(embedding_dim=50, hidden_dim=32)
    classifier.build_model(num_classes=3, vocab_size=100)
    
    # æ¨¡æ‹Ÿæ•°æ®
    texts = [
        "è¿™éƒ¨ç”µå½±çœŸçš„å¾ˆå¥½çœ‹",
        "ç”µå½±æƒ…èŠ‚å¾ˆæ— èŠ",
        "æ¼”å‘˜è¡¨æ¼”å¾ˆä¸€èˆ¬",
        "ç‰¹æ•ˆåˆ¶ä½œå¾ˆç²¾å½©",
        "å‰§æƒ…å‘å±•å¾ˆç¼“æ…¢"
    ]
    
    labels = ["æ­£é¢", "è´Ÿé¢", "ä¸­æ€§"]
    
    print(f"åˆ†ç±»ç±»åˆ«: {labels}")
    print(f"\næ–‡æœ¬åˆ†ç±»ç»“æœ:")
    
    for i, text in enumerate(texts):
        # ç®€åŒ–çš„tokenåŒ–
        token_ids = [hash(char) % 90 + 10 for char in text if char.strip()]
        
        predicted_class, probs = classifier.predict(token_ids)
        predicted_label = labels[predicted_class]
        confidence = max(probs)
        
        print(f"  æ–‡æœ¬: '{text}'")
        print(f"  é¢„æµ‹: {predicted_label} (ç½®ä¿¡åº¦: {confidence:.2f})")
        print(f"  æ¦‚ç‡åˆ†å¸ƒ: {[f'{label}:{prob:.2f}' for label, prob in zip(labels, probs)]}")
        print()

def visualize_embedding_space():
    """å¯è§†åŒ–è¯å‘é‡ç©ºé—´"""
    print("\n" + "="*50)
    print("è¯å‘é‡ç©ºé—´å¯è§†åŒ–")
    print("="*50)
    
    # åˆ›å»ºä¸€äº›ç¤ºä¾‹è¯å‘é‡
    words = ['å›½ç‹', 'å¥³ç‹', 'ç”·äºº', 'å¥³äºº', 'è‹¹æœ', 'æ©˜å­', 'æ±½è½¦', 'é£æœº']
    embeddings = np.random.randn(len(words), 2)  # 2Dç”¨äºå¯è§†åŒ–
    
    # æ‰‹åŠ¨è°ƒæ•´ä¸€äº›å‘é‡ä»¥æ˜¾ç¤ºå…³ç³»
    embeddings[0] = [1, 1]    # å›½ç‹
    embeddings[1] = [1, -1]   # å¥³ç‹  
    embeddings[2] = [0.8, 0.9]  # ç”·äºº
    embeddings[3] = [0.8, -0.9] # å¥³äºº
    embeddings[4] = [-1, 0]   # è‹¹æœ
    embeddings[5] = [-0.8, 0.2] # æ©˜å­
    embeddings[6] = [0, 1.2]   # æ±½è½¦
    embeddings[7] = [0.2, 1.5] # é£æœº
    
    plt.figure(figsize=(10, 8))
    plt.scatter(embeddings[:, 0], embeddings[:, 1], s=100, alpha=0.7)
    
    for i, word in enumerate(words):
        plt.annotate(word, (embeddings[i, 0], embeddings[i, 1]), 
                    xytext=(5, 5), textcoords='offset points',
                    fontsize=12, ha='left')
    
    # ç»˜åˆ¶ä¸€äº›å…³ç³»å‘é‡
    # æ€§åˆ«å…³ç³»: å›½ç‹->å¥³ç‹, ç”·äºº->å¥³äºº
    plt.arrow(embeddings[0, 0], embeddings[0, 1], 
             embeddings[1, 0] - embeddings[0, 0], embeddings[1, 1] - embeddings[0, 1],
             head_width=0.05, head_length=0.05, fc='red', ec='red', alpha=0.6)
    plt.arrow(embeddings[2, 0], embeddings[2, 1], 
             embeddings[3, 0] - embeddings[2, 0], embeddings[3, 1] - embeddings[2, 1],
             head_width=0.05, head_length=0.05, fc='red', ec='red', alpha=0.6)
    
    plt.xlabel('ç»´åº¦ 1')
    plt.ylabel('ç»´åº¦ 2')
    plt.title('è¯å‘é‡ç©ºé—´å¯è§†åŒ–\n(çº¢è‰²ç®­å¤´è¡¨ç¤ºæ€§åˆ«å…³ç³»)')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('word_embedding_space.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("è¯å‘é‡ç©ºé—´å›¾å·²ä¿å­˜ä¸º 'word_embedding_space.png'")

def run_comprehensive_demo():
    """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
    print("ğŸ¤– ç¬¬24ç« ï¼šæ·±åº¦è‡ªç„¶è¯­è¨€å¤„ç† - å®Œæ•´æ¼”ç¤º")
    print("="*60)
    
    # è¿è¡Œå„ä¸ªæ¼”ç¤º
    demo_word_embeddings()
    demo_rnn_text_generation()
    demo_attention_mechanism()
    demo_transformer()
    demo_bert_like()
    demo_text_classification()
    visualize_embedding_space()
    
    print("\n" + "="*60)
    print("æ·±åº¦è‡ªç„¶è¯­è¨€å¤„ç†æ¼”ç¤ºå®Œæˆï¼")
    print("="*60)
    print("\nğŸ“š å­¦ä¹ è¦ç‚¹:")
    print("â€¢ è¯å‘é‡å°†è¯æ±‡æ˜ å°„åˆ°è¿ç»­å‘é‡ç©ºé—´")
    print("â€¢ RNNå¯ä»¥å¤„ç†åºåˆ—æ•°æ®ä½†å­˜åœ¨é•¿æœŸä¾èµ–é—®é¢˜")
    print("â€¢ æ³¨æ„åŠ›æœºåˆ¶è§£å†³äº†ä¿¡æ¯ç“¶é¢ˆé—®é¢˜")
    print("â€¢ Transformeré€šè¿‡è‡ªæ³¨æ„åŠ›å®ç°å¹¶è¡ŒåŒ–")
    print("â€¢ BERTç­‰é¢„è®­ç»ƒæ¨¡å‹å¼€åˆ›äº†NLPæ–°èŒƒå¼")
    print("â€¢ æ·±åº¦å­¦ä¹ æå¤§æå‡äº†NLPä»»åŠ¡æ€§èƒ½")

if __name__ == "__main__":
    run_comprehensive_demo() 