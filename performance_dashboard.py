"""
æ€§èƒ½ä»ªè¡¨æ¿ - ç®—æ³•æ€§èƒ½ç›‘æ§å’Œæ¯”è¾ƒ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import time
import psutil
import threading
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
import json
from pathlib import Path

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    algorithm_name: str
    execution_time: float
    memory_usage: float
    cpu_usage: float
    accuracy: float
    throughput: float
    error_rate: float
    timestamp: datetime

class PerformanceDashboard:
    """æ€§èƒ½ä»ªè¡¨æ¿"""
    
    def __init__(self):
        self.metrics_history = []
        self.real_time_data = []
        self.benchmark_results = {}
        self.monitoring_active = False
        self.monitoring_thread = None
        
    def start_monitoring(self, interval: float = 1.0):
        """å¼€å§‹å®æ—¶ç›‘æ§"""
        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(
            target=self._monitor_system, 
            args=(interval,)
        )
        self.monitoring_thread.start()
    
    def stop_monitoring(self):
        """åœæ­¢å®æ—¶ç›‘æ§"""
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join()
    
    def _monitor_system(self, interval: float):
        """ç³»ç»Ÿç›‘æ§çº¿ç¨‹"""
        while self.monitoring_active:
            # è·å–ç³»ç»ŸæŒ‡æ ‡
            cpu_percent = psutil.cpu_percent(interval=0.1)
            memory_info = psutil.virtual_memory()
            
            # è®°å½•å®æ—¶æ•°æ®
            self.real_time_data.append({
                'timestamp': datetime.now(),
                'cpu_usage': cpu_percent,
                'memory_usage': memory_info.percent,
                'memory_available': memory_info.available / (1024**3),  # GB
                'memory_total': memory_info.total / (1024**3)  # GB
            })
            
            # ä¿æŒæœ€è¿‘1000ä¸ªæ•°æ®ç‚¹
            if len(self.real_time_data) > 1000:
                self.real_time_data.pop(0)
            
            time.sleep(interval)
    
    def add_metrics(self, metrics: PerformanceMetrics):
        """æ·»åŠ æ€§èƒ½æŒ‡æ ‡"""
        self.metrics_history.append(metrics)
    
    def run_benchmark(self, algorithms: Dict[str, Any], 
                     test_data: Dict[str, Any],
                     iterations: int = 10) -> Dict[str, Any]:
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        print("ğŸ å¼€å§‹åŸºå‡†æµ‹è¯•...")
        
        results = {}
        
        for alg_name, algorithm in algorithms.items():
            print(f"  æµ‹è¯• {alg_name}...")
            
            alg_results = {
                'execution_times': [],
                'memory_usage': [],
                'cpu_usage': [],
                'accuracy': [],
                'throughput': []
            }
            
            for i in range(iterations):
                # æ¸…ç†å†…å­˜
                import gc
                gc.collect()
                
                # è®°å½•åˆå§‹çŠ¶æ€
                initial_memory = psutil.virtual_memory().used / (1024**2)  # MB
                initial_cpu = psutil.cpu_percent(interval=0.1)
                
                # è¿è¡Œç®—æ³•
                start_time = time.time()
                result = self._run_algorithm(algorithm, test_data)
                end_time = time.time()
                
                # è®°å½•ç»“æœ
                execution_time = end_time - start_time
                final_memory = psutil.virtual_memory().used / (1024**2)  # MB
                memory_used = final_memory - initial_memory
                final_cpu = psutil.cpu_percent(interval=0.1)
                
                alg_results['execution_times'].append(execution_time)
                alg_results['memory_usage'].append(memory_used)
                alg_results['cpu_usage'].append(final_cpu - initial_cpu)
                
                if 'accuracy' in result:
                    alg_results['accuracy'].append(result['accuracy'])
                
                if execution_time > 0:
                    throughput = len(test_data.get('X', [])) / execution_time
                    alg_results['throughput'].append(throughput)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            results[alg_name] = {
                'mean_execution_time': np.mean(alg_results['execution_times']),
                'std_execution_time': np.std(alg_results['execution_times']),
                'mean_memory_usage': np.mean(alg_results['memory_usage']),
                'std_memory_usage': np.std(alg_results['memory_usage']),
                'mean_cpu_usage': np.mean(alg_results['cpu_usage']),
                'std_cpu_usage': np.std(alg_results['cpu_usage']),
                'mean_accuracy': np.mean(alg_results['accuracy']) if alg_results['accuracy'] else 0,
                'std_accuracy': np.std(alg_results['accuracy']) if alg_results['accuracy'] else 0,
                'mean_throughput': np.mean(alg_results['throughput']) if alg_results['throughput'] else 0,
                'std_throughput': np.std(alg_results['throughput']) if alg_results['throughput'] else 0,
                'raw_data': alg_results
            }
        
        self.benchmark_results = results
        return results
    
    def _run_algorithm(self, algorithm: Any, test_data: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œç®—æ³•"""
        # è¿™é‡Œåº”è¯¥æ ¹æ®ç®—æ³•ç±»å‹è°ƒç”¨ç›¸åº”çš„æ–¹æ³•
        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿä¸€ä¸ªç®€å•çš„æ‰§è¡Œ
        
        if hasattr(algorithm, 'fit') and hasattr(algorithm, 'predict'):
            # æœºå™¨å­¦ä¹ ç®—æ³•
            if 'X' in test_data and 'y' in test_data:
                algorithm.fit(test_data['X'], test_data['y'])
                predictions = algorithm.predict(test_data['X'])
                accuracy = np.mean(predictions == test_data['y'])
                return {'accuracy': accuracy}
        
        elif hasattr(algorithm, 'run'):
            # æœç´¢ç®—æ³•
            if 'problem' in test_data:
                result = algorithm.run(test_data['problem'])
                return result
        
        # é»˜è®¤æƒ…å†µ
        time.sleep(0.001)  # æ¨¡æ‹Ÿè®¡ç®—
        return {'accuracy': np.random.uniform(0.7, 0.95)}
    
    def create_performance_comparison_chart(self) -> go.Figure:
        """åˆ›å»ºæ€§èƒ½æ¯”è¾ƒå›¾è¡¨"""
        if not self.benchmark_results:
            return go.Figure()
        
        algorithms = list(self.benchmark_results.keys())
        
        # åˆ›å»ºå­å›¾
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('æ‰§è¡Œæ—¶é—´', 'å†…å­˜ä½¿ç”¨', 'å‡†ç¡®ç‡', 'ååé‡'),
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}]]
        )
        
        # æ‰§è¡Œæ—¶é—´
        execution_times = [self.benchmark_results[alg]['mean_execution_time'] for alg in algorithms]
        execution_stds = [self.benchmark_results[alg]['std_execution_time'] for alg in algorithms]
        
        fig.add_trace(
            go.Bar(
                x=algorithms,
                y=execution_times,
                error_y=dict(type='data', array=execution_stds),
                name='æ‰§è¡Œæ—¶é—´',
                showlegend=False
            ),
            row=1, col=1
        )
        
        # å†…å­˜ä½¿ç”¨
        memory_usage = [self.benchmark_results[alg]['mean_memory_usage'] for alg in algorithms]
        memory_stds = [self.benchmark_results[alg]['std_memory_usage'] for alg in algorithms]
        
        fig.add_trace(
            go.Bar(
                x=algorithms,
                y=memory_usage,
                error_y=dict(type='data', array=memory_stds),
                name='å†…å­˜ä½¿ç”¨',
                showlegend=False
            ),
            row=1, col=2
        )
        
        # å‡†ç¡®ç‡
        accuracy = [self.benchmark_results[alg]['mean_accuracy'] for alg in algorithms]
        accuracy_stds = [self.benchmark_results[alg]['std_accuracy'] for alg in algorithms]
        
        fig.add_trace(
            go.Bar(
                x=algorithms,
                y=accuracy,
                error_y=dict(type='data', array=accuracy_stds),
                name='å‡†ç¡®ç‡',
                showlegend=False
            ),
            row=2, col=1
        )
        
        # ååé‡
        throughput = [self.benchmark_results[alg]['mean_throughput'] for alg in algorithms]
        throughput_stds = [self.benchmark_results[alg]['std_throughput'] for alg in algorithms]
        
        fig.add_trace(
            go.Bar(
                x=algorithms,
                y=throughput,
                error_y=dict(type='data', array=throughput_stds),
                name='ååé‡',
                showlegend=False
            ),
            row=2, col=2
        )
        
        fig.update_layout(
            title_text="ç®—æ³•æ€§èƒ½æ¯”è¾ƒ",
            height=600,
            showlegend=False
        )
        
        # æ›´æ–°è½´æ ‡ç­¾
        fig.update_xaxes(title_text="ç®—æ³•", row=1, col=1)
        fig.update_xaxes(title_text="ç®—æ³•", row=1, col=2)
        fig.update_xaxes(title_text="ç®—æ³•", row=2, col=1)
        fig.update_xaxes(title_text="ç®—æ³•", row=2, col=2)
        
        fig.update_yaxes(title_text="æ—¶é—´(ç§’)", row=1, col=1)
        fig.update_yaxes(title_text="å†…å­˜(MB)", row=1, col=2)
        fig.update_yaxes(title_text="å‡†ç¡®ç‡", row=2, col=1)
        fig.update_yaxes(title_text="æ ·æœ¬/ç§’", row=2, col=2)
        
        return fig
    
    def create_real_time_monitoring_chart(self) -> go.Figure:
        """åˆ›å»ºå®æ—¶ç›‘æ§å›¾è¡¨"""
        if not self.real_time_data:
            return go.Figure()
        
        df = pd.DataFrame(self.real_time_data)
        
        fig = make_subplots(
            rows=2, cols=1,
            subplot_titles=('CPUä½¿ç”¨ç‡', 'å†…å­˜ä½¿ç”¨'),
            shared_xaxes=True,
            vertical_spacing=0.1
        )
        
        # CPUä½¿ç”¨ç‡
        fig.add_trace(
            go.Scatter(
                x=df['timestamp'],
                y=df['cpu_usage'],
                mode='lines',
                name='CPUä½¿ç”¨ç‡',
                line=dict(color='blue', width=2)
            ),
            row=1, col=1
        )
        
        # å†…å­˜ä½¿ç”¨
        fig.add_trace(
            go.Scatter(
                x=df['timestamp'],
                y=df['memory_usage'],
                mode='lines',
                name='å†…å­˜ä½¿ç”¨ç‡',
                line=dict(color='red', width=2)
            ),
            row=2, col=1
        )
        
        fig.update_layout(
            title_text="å®æ—¶ç³»ç»Ÿç›‘æ§",
            height=500,
            showlegend=True
        )
        
        fig.update_xaxes(title_text="æ—¶é—´", row=2, col=1)
        fig.update_yaxes(title_text="CPUä½¿ç”¨ç‡ (%)", row=1, col=1)
        fig.update_yaxes(title_text="å†…å­˜ä½¿ç”¨ç‡ (%)", row=2, col=1)
        
        return fig
    
    def create_performance_trend_chart(self) -> go.Figure:
        """åˆ›å»ºæ€§èƒ½è¶‹åŠ¿å›¾è¡¨"""
        if not self.metrics_history:
            return go.Figure()
        
        df = pd.DataFrame([{
            'timestamp': m.timestamp,
            'algorithm': m.algorithm_name,
            'execution_time': m.execution_time,
            'memory_usage': m.memory_usage,
            'accuracy': m.accuracy,
            'throughput': m.throughput
        } for m in self.metrics_history])
        
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('æ‰§è¡Œæ—¶é—´è¶‹åŠ¿', 'å†…å­˜ä½¿ç”¨è¶‹åŠ¿', 'å‡†ç¡®ç‡è¶‹åŠ¿', 'ååé‡è¶‹åŠ¿'),
            shared_xaxes=True
        )
        
        algorithms = df['algorithm'].unique()
        colors = px.colors.qualitative.Set1[:len(algorithms)]
        
        for i, alg in enumerate(algorithms):
            alg_data = df[df['algorithm'] == alg]
            
            # æ‰§è¡Œæ—¶é—´
            fig.add_trace(
                go.Scatter(
                    x=alg_data['timestamp'],
                    y=alg_data['execution_time'],
                    mode='lines+markers',
                    name=alg,
                    line=dict(color=colors[i]),
                    showlegend=True
                ),
                row=1, col=1
            )
            
            # å†…å­˜ä½¿ç”¨
            fig.add_trace(
                go.Scatter(
                    x=alg_data['timestamp'],
                    y=alg_data['memory_usage'],
                    mode='lines+markers',
                    name=alg,
                    line=dict(color=colors[i]),
                    showlegend=False
                ),
                row=1, col=2
            )
            
            # å‡†ç¡®ç‡
            fig.add_trace(
                go.Scatter(
                    x=alg_data['timestamp'],
                    y=alg_data['accuracy'],
                    mode='lines+markers',
                    name=alg,
                    line=dict(color=colors[i]),
                    showlegend=False
                ),
                row=2, col=1
            )
            
            # ååé‡
            fig.add_trace(
                go.Scatter(
                    x=alg_data['timestamp'],
                    y=alg_data['throughput'],
                    mode='lines+markers',
                    name=alg,
                    line=dict(color=colors[i]),
                    showlegend=False
                ),
                row=2, col=2
            )
        
        fig.update_layout(
            title_text="ç®—æ³•æ€§èƒ½è¶‹åŠ¿",
            height=600
        )
        
        fig.update_yaxes(title_text="æ—¶é—´(ç§’)", row=1, col=1)
        fig.update_yaxes(title_text="å†…å­˜(MB)", row=1, col=2)
        fig.update_yaxes(title_text="å‡†ç¡®ç‡", row=2, col=1)
        fig.update_yaxes(title_text="æ ·æœ¬/ç§’", row=2, col=2)
        
        return fig
    
    def create_algorithm_ranking_chart(self) -> go.Figure:
        """åˆ›å»ºç®—æ³•æ’åå›¾è¡¨"""
        if not self.benchmark_results:
            return go.Figure()
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        algorithms = list(self.benchmark_results.keys())
        scores = []
        
        for alg in algorithms:
            result = self.benchmark_results[alg]
            
            # å½’ä¸€åŒ–å„æŒ‡æ ‡ï¼ˆè¶Šå°è¶Šå¥½çš„æŒ‡æ ‡éœ€è¦å–å€’æ•°ï¼‰
            time_score = 1 / (result['mean_execution_time'] + 1e-6)
            memory_score = 1 / (result['mean_memory_usage'] + 1e-6)
            accuracy_score = result['mean_accuracy']
            throughput_score = result['mean_throughput']
            
            # åŠ æƒç»¼åˆè¯„åˆ†
            total_score = (time_score * 0.3 + memory_score * 0.2 + 
                          accuracy_score * 0.3 + throughput_score * 0.2)
            
            scores.append({
                'algorithm': alg,
                'time_score': time_score,
                'memory_score': memory_score,
                'accuracy_score': accuracy_score,
                'throughput_score': throughput_score,
                'total_score': total_score
            })
        
        # æ’åº
        scores.sort(key=lambda x: x['total_score'], reverse=True)
        
        df = pd.DataFrame(scores)
        
        fig = go.Figure()
        
        # å †å æ¡å½¢å›¾
        fig.add_trace(go.Bar(
            x=df['algorithm'],
            y=df['time_score'],
            name='æ—¶é—´æ•ˆç‡',
            marker_color='lightblue'
        ))
        
        fig.add_trace(go.Bar(
            x=df['algorithm'],
            y=df['memory_score'],
            name='å†…å­˜æ•ˆç‡',
            marker_color='lightgreen'
        ))
        
        fig.add_trace(go.Bar(
            x=df['algorithm'],
            y=df['accuracy_score'],
            name='å‡†ç¡®ç‡',
            marker_color='lightcoral'
        ))
        
        fig.add_trace(go.Bar(
            x=df['algorithm'],
            y=df['throughput_score'],
            name='ååé‡',
            marker_color='lightyellow'
        ))
        
        fig.update_layout(
            title='ç®—æ³•ç»¼åˆæ€§èƒ½æ’å',
            xaxis_title='ç®—æ³•',
            yaxis_title='è¯„åˆ†',
            barmode='stack',
            height=500
        )
        
        return fig
    
    def generate_performance_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        if not self.benchmark_results:
            return "æ²¡æœ‰å¯ç”¨çš„åŸºå‡†æµ‹è¯•ç»“æœ"
        
        report = []
        report.append("# ç®—æ³•æ€§èƒ½åˆ†ææŠ¥å‘Š")
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # ç®—æ³•æ¦‚è§ˆ
        report.append("## ç®—æ³•æ¦‚è§ˆ")
        report.append(f"æµ‹è¯•ç®—æ³•æ•°é‡: {len(self.benchmark_results)}")
        report.append(f"ç®—æ³•åˆ—è¡¨: {', '.join(self.benchmark_results.keys())}")
        report.append("")
        
        # æ€§èƒ½æ’å
        algorithms = list(self.benchmark_results.keys())
        
        # æŒ‰æ‰§è¡Œæ—¶é—´æ’åº
        time_ranking = sorted(algorithms, 
                             key=lambda x: self.benchmark_results[x]['mean_execution_time'])
        
        report.append("## æ€§èƒ½æ’å")
        report.append("")
        report.append("### æ‰§è¡Œæ—¶é—´æ’å (ä»å¿«åˆ°æ…¢)")
        for i, alg in enumerate(time_ranking, 1):
            time_val = self.benchmark_results[alg]['mean_execution_time']
            report.append(f"{i}. {alg}: {time_val:.4f} ç§’")
        
        report.append("")
        
        # å†…å­˜ä½¿ç”¨æ’å
        memory_ranking = sorted(algorithms, 
                               key=lambda x: self.benchmark_results[x]['mean_memory_usage'])
        
        report.append("### å†…å­˜ä½¿ç”¨æ’å (ä»ä½åˆ°é«˜)")
        for i, alg in enumerate(memory_ranking, 1):
            memory_val = self.benchmark_results[alg]['mean_memory_usage']
            report.append(f"{i}. {alg}: {memory_val:.2f} MB")
        
        report.append("")
        
        # å‡†ç¡®ç‡æ’å
        accuracy_ranking = sorted(algorithms, 
                                 key=lambda x: self.benchmark_results[x]['mean_accuracy'], 
                                 reverse=True)
        
        report.append("### å‡†ç¡®ç‡æ’å (ä»é«˜åˆ°ä½)")
        for i, alg in enumerate(accuracy_ranking, 1):
            accuracy_val = self.benchmark_results[alg]['mean_accuracy']
            report.append(f"{i}. {alg}: {accuracy_val:.3f}")
        
        report.append("")
        
        # è¯¦ç»†ç»Ÿè®¡
        report.append("## è¯¦ç»†ç»Ÿè®¡")
        report.append("")
        
        for alg in algorithms:
            result = self.benchmark_results[alg]
            report.append(f"### {alg}")
            report.append(f"- å¹³å‡æ‰§è¡Œæ—¶é—´: {result['mean_execution_time']:.4f} Â± {result['std_execution_time']:.4f} ç§’")
            report.append(f"- å¹³å‡å†…å­˜ä½¿ç”¨: {result['mean_memory_usage']:.2f} Â± {result['std_memory_usage']:.2f} MB")
            report.append(f"- å¹³å‡å‡†ç¡®ç‡: {result['mean_accuracy']:.3f} Â± {result['std_accuracy']:.3f}")
            report.append(f"- å¹³å‡ååé‡: {result['mean_throughput']:.2f} Â± {result['std_throughput']:.2f} æ ·æœ¬/ç§’")
            report.append("")
        
        # å»ºè®®
        report.append("## æ€§èƒ½å»ºè®®")
        report.append("")
        
        fastest_alg = time_ranking[0]
        most_accurate_alg = accuracy_ranking[0]
        least_memory_alg = memory_ranking[0]
        
        report.append(f"- **æœ€å¿«ç®—æ³•**: {fastest_alg}")
        report.append(f"- **æœ€å‡†ç¡®ç®—æ³•**: {most_accurate_alg}")
        report.append(f"- **æœ€çœå†…å­˜ç®—æ³•**: {least_memory_alg}")
        
        if fastest_alg == most_accurate_alg:
            report.append(f"- **æ¨èç®—æ³•**: {fastest_alg} (é€Ÿåº¦å’Œå‡†ç¡®ç‡å‡æœ€ä¼˜)")
        else:
            report.append(f"- **é€Ÿåº¦ä¼˜å…ˆ**: é€‰æ‹© {fastest_alg}")
            report.append(f"- **å‡†ç¡®ç‡ä¼˜å…ˆ**: é€‰æ‹© {most_accurate_alg}")
        
        return "\n".join(report)
    
    def save_results(self, filename: str = "performance_results.json"):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        data = {
            'benchmark_results': self.benchmark_results,
            'metrics_history': [
                {
                    'algorithm_name': m.algorithm_name,
                    'execution_time': m.execution_time,
                    'memory_usage': m.memory_usage,
                    'cpu_usage': m.cpu_usage,
                    'accuracy': m.accuracy,
                    'throughput': m.throughput,
                    'error_rate': m.error_rate,
                    'timestamp': m.timestamp.isoformat()
                }
                for m in self.metrics_history
            ],
            'real_time_data': [
                {
                    'timestamp': d['timestamp'].isoformat(),
                    'cpu_usage': d['cpu_usage'],
                    'memory_usage': d['memory_usage'],
                    'memory_available': d['memory_available'],
                    'memory_total': d['memory_total']
                }
                for d in self.real_time_data
            ]
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"ç»“æœå·²ä¿å­˜åˆ° {filename}")
    
    def load_results(self, filename: str = "performance_results.json"):
        """ä»æ–‡ä»¶åŠ è½½ç»“æœ"""
        if not Path(filename).exists():
            print(f"æ–‡ä»¶ {filename} ä¸å­˜åœ¨")
            return
        
        with open(filename, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.benchmark_results = data.get('benchmark_results', {})
        
        # åŠ è½½æŒ‡æ ‡å†å²
        self.metrics_history = []
        for m in data.get('metrics_history', []):
            self.metrics_history.append(PerformanceMetrics(
                algorithm_name=m['algorithm_name'],
                execution_time=m['execution_time'],
                memory_usage=m['memory_usage'],
                cpu_usage=m['cpu_usage'],
                accuracy=m['accuracy'],
                throughput=m['throughput'],
                error_rate=m['error_rate'],
                timestamp=datetime.fromisoformat(m['timestamp'])
            ))
        
        # åŠ è½½å®æ—¶æ•°æ®
        self.real_time_data = []
        for d in data.get('real_time_data', []):
            self.real_time_data.append({
                'timestamp': datetime.fromisoformat(d['timestamp']),
                'cpu_usage': d['cpu_usage'],
                'memory_usage': d['memory_usage'],
                'memory_available': d['memory_available'],
                'memory_total': d['memory_total']
            })
        
        print(f"ç»“æœå·²ä» {filename} åŠ è½½")

# æ¨¡æ‹Ÿç®—æ³•ç±»
class MockAlgorithm:
    """æ¨¡æ‹Ÿç®—æ³•ç±»"""
    
    def __init__(self, name: str, base_time: float = 1.0, 
                 base_memory: float = 10.0, base_accuracy: float = 0.85):
        self.name = name
        self.base_time = base_time
        self.base_memory = base_memory
        self.base_accuracy = base_accuracy
    
    def fit(self, X, y):
        """è®­ç»ƒ"""
        time.sleep(self.base_time * np.random.uniform(0.8, 1.2))
    
    def predict(self, X):
        """é¢„æµ‹"""
        # æ¨¡æ‹Ÿé¢„æµ‹ç»“æœ
        return np.random.randint(0, 2, len(X))

# æ¼”ç¤ºå‡½æ•°
def demo_performance_dashboard():
    """æ¼”ç¤ºæ€§èƒ½ä»ªè¡¨æ¿"""
    print("ğŸ“Š æ€§èƒ½ä»ªè¡¨æ¿æ¼”ç¤º")
    print("="*50)
    
    # åˆ›å»ºä»ªè¡¨æ¿
    dashboard = PerformanceDashboard()
    
    # åˆ›å»ºæ¨¡æ‹Ÿç®—æ³•
    algorithms = {
        'FastAlgorithm': MockAlgorithm('FastAlgorithm', base_time=0.5, base_accuracy=0.80),
        'AccurateAlgorithm': MockAlgorithm('AccurateAlgorithm', base_time=2.0, base_accuracy=0.95),
        'BalancedAlgorithm': MockAlgorithm('BalancedAlgorithm', base_time=1.0, base_accuracy=0.88),
        'MemoryEfficientAlgorithm': MockAlgorithm('MemoryEfficientAlgorithm', 
                                                  base_time=1.5, base_memory=5.0, base_accuracy=0.85)
    }
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    np.random.seed(42)
    test_data = {
        'X': np.random.randn(1000, 10),
        'y': np.random.randint(0, 2, 1000)
    }
    
    # å¯åŠ¨å®æ—¶ç›‘æ§
    print("ğŸ” å¯åŠ¨å®æ—¶ç›‘æ§...")
    dashboard.start_monitoring(interval=0.5)
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    print("ğŸ è¿è¡ŒåŸºå‡†æµ‹è¯•...")
    benchmark_results = dashboard.run_benchmark(algorithms, test_data, iterations=5)
    
    # åœæ­¢ç›‘æ§
    dashboard.stop_monitoring()
    
    # æ˜¾ç¤ºç»“æœ
    print("\nğŸ“ˆ åŸºå‡†æµ‹è¯•ç»“æœ:")
    for alg_name, result in benchmark_results.items():
        print(f"\n{alg_name}:")
        print(f"  å¹³å‡æ‰§è¡Œæ—¶é—´: {result['mean_execution_time']:.4f} Â± {result['std_execution_time']:.4f} ç§’")
        print(f"  å¹³å‡å†…å­˜ä½¿ç”¨: {result['mean_memory_usage']:.2f} Â± {result['std_memory_usage']:.2f} MB")
        print(f"  å¹³å‡å‡†ç¡®ç‡: {result['mean_accuracy']:.3f} Â± {result['std_accuracy']:.3f}")
        print(f"  å¹³å‡ååé‡: {result['mean_throughput']:.2f} Â± {result['std_throughput']:.2f} æ ·æœ¬/ç§’")
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“ ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š...")
    report = dashboard.generate_performance_report()
    
    # ä¿å­˜æŠ¥å‘Š
    with open("performance_report.md", "w", encoding="utf-8") as f:
        f.write(report)
    
    print("æŠ¥å‘Šå·²ä¿å­˜åˆ° performance_report.md")
    
    # ä¿å­˜ç»“æœ
    dashboard.save_results("performance_results.json")
    
    print("\nâœ… æ¼”ç¤ºå®Œæˆï¼")

if __name__ == "__main__":
    demo_performance_dashboard() 