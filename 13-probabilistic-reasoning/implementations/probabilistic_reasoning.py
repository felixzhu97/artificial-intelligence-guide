#!/usr/bin/env python3
"""
ç¬¬13ç« ï¼šæ¦‚ç‡æ¨ç† (Probabilistic Reasoning)

æœ¬æ¨¡å—å®ç°äº†æ¦‚ç‡æ¨ç†çš„æ ¸å¿ƒæ¦‚å¿µï¼š
- è´å¶æ–¯æ¨ç†
- é©¬å°”å¯å¤«æ¨¡å‹
- æ¦‚ç‡åˆ†å¸ƒ
- æ¡ä»¶æ¦‚ç‡
- ä¸ç¡®å®šæ€§æ¨ç†
"""

import numpy as np
import matplotlib.pyplot as plt
import math
from typing import Dict, List, Tuple, Any, Optional, Set
from dataclasses import dataclass
from collections import defaultdict
import itertools

plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

class ProbabilityDistribution:
    """æ¦‚ç‡åˆ†å¸ƒåŸºç±»"""
    
    def __init__(self, variables: List[str]):
        self.variables = variables
        self.probabilities: Dict[Tuple, float] = {}
    
    def set_probability(self, assignment: Dict[str, Any], probability: float):
        """è®¾ç½®æ¦‚ç‡å€¼"""
        key = tuple(assignment[var] for var in self.variables)
        self.probabilities[key] = probability
    
    def get_probability(self, assignment: Dict[str, Any]) -> float:
        """è·å–æ¦‚ç‡å€¼"""
        key = tuple(assignment[var] for var in self.variables)
        return self.probabilities.get(key, 0.0)
    
    def normalize(self):
        """å½’ä¸€åŒ–æ¦‚ç‡åˆ†å¸ƒ"""
        total = sum(self.probabilities.values())
        if total > 0:
            for key in self.probabilities:
                self.probabilities[key] /= total
    
    def marginalize(self, variable: str) -> 'ProbabilityDistribution':
        """è¾¹é™…åŒ–ï¼ˆæ¶ˆé™¤å˜é‡ï¼‰"""
        remaining_vars = [v for v in self.variables if v != variable]
        marginal = ProbabilityDistribution(remaining_vars)
        
        # è·å–è¢«è¾¹é™…åŒ–å˜é‡çš„æ‰€æœ‰å¯èƒ½å€¼
        values = set()
        for key in self.probabilities:
            var_index = self.variables.index(variable)
            values.add(key[var_index])
        
        # è®¡ç®—è¾¹é™…æ¦‚ç‡
        for key in self.probabilities:
            marginal_key = tuple(key[i] for i, var in enumerate(self.variables) if var != variable)
            if marginal_key not in marginal.probabilities:
                marginal.probabilities[marginal_key] = 0
            marginal.probabilities[marginal_key] += self.probabilities[key]
        
        return marginal

class ConditionalProbabilityTable:
    """æ¡ä»¶æ¦‚ç‡è¡¨"""
    
    def __init__(self, variable: str, parents: List[str]):
        self.variable = variable
        self.parents = parents
        self.table: Dict[Tuple, Dict[Any, float]] = {}
    
    def set_probability(self, parent_assignment: Dict[str, Any], variable_value: Any, probability: float):
        """è®¾ç½®æ¡ä»¶æ¦‚ç‡"""
        parent_key = tuple(parent_assignment[parent] for parent in self.parents)
        if parent_key not in self.table:
            self.table[parent_key] = {}
        self.table[parent_key][variable_value] = probability
    
    def get_probability(self, parent_assignment: Dict[str, Any], variable_value: Any) -> float:
        """è·å–æ¡ä»¶æ¦‚ç‡"""
        parent_key = tuple(parent_assignment[parent] for parent in self.parents)
        return self.table.get(parent_key, {}).get(variable_value, 0.0)
    
    def get_distribution(self, parent_assignment: Dict[str, Any]) -> Dict[Any, float]:
        """è·å–ç»™å®šçˆ¶èŠ‚ç‚¹å€¼çš„æ¡ä»¶åˆ†å¸ƒ"""
        parent_key = tuple(parent_assignment[parent] for parent in self.parents)
        return self.table.get(parent_key, {})

class BayesianNetwork:
    """è´å¶æ–¯ç½‘ç»œ"""
    
    def __init__(self):
        self.variables: Set[str] = set()
        self.parents: Dict[str, List[str]] = {}
        self.cpts: Dict[str, ConditionalProbabilityTable] = {}
        self.domains: Dict[str, List[Any]] = {}
    
    def add_variable(self, variable: str, domain: List[Any], parents: List[str] = None):
        """æ·»åŠ å˜é‡"""
        self.variables.add(variable)
        self.domains[variable] = domain
        self.parents[variable] = parents or []
        self.cpts[variable] = ConditionalProbabilityTable(variable, self.parents[variable])
    
    def set_probability(self, variable: str, parent_assignment: Dict[str, Any], variable_value: Any, probability: float):
        """è®¾ç½®æ¡ä»¶æ¦‚ç‡"""
        self.cpts[variable].set_probability(parent_assignment, variable_value, probability)
    
    def query(self, query_var: str, evidence: Dict[str, Any] = None) -> Dict[Any, float]:
        """æŸ¥è¯¢æ¦‚ç‡ï¼ˆæšä¸¾æ¨ç†ï¼‰"""
        evidence = evidence or {}
        
        # è·å–æ‰€æœ‰å¯èƒ½çš„å®Œå…¨èµ‹å€¼
        all_vars = list(self.variables)
        all_assignments = []
        
        for assignment in itertools.product(*[self.domains[var] for var in all_vars]):
            assignment_dict = {var: val for var, val in zip(all_vars, assignment)}
            
            # æ£€æŸ¥æ˜¯å¦ä¸è¯æ®ä¸€è‡´
            consistent = True
            for var, val in evidence.items():
                if assignment_dict[var] != val:
                    consistent = False
                    break
            
            if consistent:
                all_assignments.append(assignment_dict)
        
        # è®¡ç®—æŸ¥è¯¢å˜é‡çš„æ¦‚ç‡åˆ†å¸ƒ
        query_distribution = {}
        for query_value in self.domains[query_var]:
            prob_sum = 0
            for assignment in all_assignments:
                if assignment[query_var] == query_value:
                    prob = self._calculate_joint_probability(assignment)
                    prob_sum += prob
            query_distribution[query_value] = prob_sum
        
        # å½’ä¸€åŒ–
        total = sum(query_distribution.values())
        if total > 0:
            for key in query_distribution:
                query_distribution[key] /= total
        
        return query_distribution
    
    def _calculate_joint_probability(self, assignment: Dict[str, Any]) -> float:
        """è®¡ç®—è”åˆæ¦‚ç‡"""
        probability = 1.0
        
        for variable in self.variables:
            parent_assignment = {parent: assignment[parent] for parent in self.parents[variable]}
            cpt = self.cpts[variable]
            prob = cpt.get_probability(parent_assignment, assignment[variable])
            probability *= prob
        
        return probability

class MarkovChain:
    """é©¬å°”å¯å¤«é“¾"""
    
    def __init__(self, states: List[str]):
        self.states = states
        self.transition_matrix = np.zeros((len(states), len(states)))
        self.state_to_index = {state: i for i, state in enumerate(states)}
        self.initial_distribution = np.zeros(len(states))
    
    def set_transition_probability(self, from_state: str, to_state: str, probability: float):
        """è®¾ç½®è½¬ç§»æ¦‚ç‡"""
        from_idx = self.state_to_index[from_state]
        to_idx = self.state_to_index[to_state]
        self.transition_matrix[from_idx, to_idx] = probability
    
    def set_initial_probability(self, state: str, probability: float):
        """è®¾ç½®åˆå§‹æ¦‚ç‡"""
        idx = self.state_to_index[state]
        self.initial_distribution[idx] = probability
    
    def get_stationary_distribution(self) -> Dict[str, float]:
        """è®¡ç®—ç¨³æ€åˆ†å¸ƒ"""
        # æ±‚è½¬ç§»çŸ©é˜µçš„å·¦ç‰¹å¾å‘é‡ï¼ˆç‰¹å¾å€¼ä¸º1ï¼‰
        eigenvalues, eigenvectors = np.linalg.eig(self.transition_matrix.T)
        
        # æ‰¾åˆ°ç‰¹å¾å€¼æœ€æ¥è¿‘1çš„ç‰¹å¾å‘é‡
        stationary_idx = np.argmin(np.abs(eigenvalues - 1))
        stationary_vector = np.real(eigenvectors[:, stationary_idx])
        
        # å½’ä¸€åŒ–
        stationary_vector = np.abs(stationary_vector)
        stationary_vector /= np.sum(stationary_vector)
        
        return {state: prob for state, prob in zip(self.states, stationary_vector)}
    
    def simulate(self, steps: int, initial_state: str = None) -> List[str]:
        """æ¨¡æ‹Ÿé©¬å°”å¯å¤«é“¾"""
        if initial_state is None:
            # æ ¹æ®åˆå§‹åˆ†å¸ƒé€‰æ‹©åˆå§‹çŠ¶æ€
            current_idx = np.random.choice(len(self.states), p=self.initial_distribution)
        else:
            current_idx = self.state_to_index[initial_state]
        
        sequence = [self.states[current_idx]]
        
        for _ in range(steps - 1):
            # æ ¹æ®è½¬ç§»æ¦‚ç‡é€‰æ‹©ä¸‹ä¸€ä¸ªçŠ¶æ€
            next_idx = np.random.choice(len(self.states), p=self.transition_matrix[current_idx])
            sequence.append(self.states[next_idx])
            current_idx = next_idx
        
        return sequence
    
    def forward_probability(self, observations: List[str], steps: int) -> np.ndarray:
        """å‰å‘æ¦‚ç‡ç®—æ³•"""
        alpha = np.zeros((steps, len(self.states)))
        
        # åˆå§‹åŒ–
        alpha[0] = self.initial_distribution
        
        # é€’æ¨
        for t in range(1, steps):
            for j in range(len(self.states)):
                alpha[t, j] = np.sum(alpha[t-1] * self.transition_matrix[:, j])
        
        return alpha

class NaiveBayesClassifier:
    """æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨"""
    
    def __init__(self):
        self.class_probabilities: Dict[str, float] = {}
        self.feature_probabilities: Dict[str, Dict[str, Dict[Any, float]]] = {}
        self.classes: Set[str] = set()
        self.features: Set[str] = set()
    
    def train(self, training_data: List[Tuple[Dict[str, Any], str]]):
        """è®­ç»ƒåˆ†ç±»å™¨"""
        # ç»Ÿè®¡ç±»åˆ«å’Œç‰¹å¾
        class_counts = defaultdict(int)
        feature_value_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))
        
        for features, class_label in training_data:
            self.classes.add(class_label)
            class_counts[class_label] += 1
            
            for feature_name, feature_value in features.items():
                self.features.add(feature_name)
                feature_value_counts[class_label][feature_name][feature_value] += 1
        
        # è®¡ç®—ç±»åˆ«æ¦‚ç‡
        total_samples = len(training_data)
        for class_label in self.classes:
            self.class_probabilities[class_label] = class_counts[class_label] / total_samples
        
        # è®¡ç®—æ¡ä»¶æ¦‚ç‡
        for class_label in self.classes:
            self.feature_probabilities[class_label] = {}
            for feature_name in self.features:
                self.feature_probabilities[class_label][feature_name] = {}
                
                # è·å–æ‰€æœ‰å¯èƒ½çš„ç‰¹å¾å€¼
                all_values = set()
                for features, _ in training_data:
                    if feature_name in features:
                        all_values.add(features[feature_name])
                
                # è®¡ç®—æ¡ä»¶æ¦‚ç‡ï¼ˆä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼‰
                total_class_samples = class_counts[class_label]
                num_values = len(all_values)
                
                for value in all_values:
                    count = feature_value_counts[class_label][feature_name][value]
                    # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
                    probability = (count + 1) / (total_class_samples + num_values)
                    self.feature_probabilities[class_label][feature_name][value] = probability
    
    def classify(self, features: Dict[str, Any]) -> Tuple[str, Dict[str, float]]:
        """åˆ†ç±»"""
        class_scores = {}
        
        for class_label in self.classes:
            # è®¡ç®—åéªŒæ¦‚ç‡ï¼ˆå¯¹æ•°ç©ºé—´é¿å…ä¸‹æº¢ï¼‰
            log_prob = np.log(self.class_probabilities[class_label])
            
            for feature_name, feature_value in features.items():
                if (class_label in self.feature_probabilities and 
                    feature_name in self.feature_probabilities[class_label] and
                    feature_value in self.feature_probabilities[class_label][feature_name]):
                    
                    feature_prob = self.feature_probabilities[class_label][feature_name][feature_value]
                    log_prob += np.log(feature_prob)
                else:
                    # å¤„ç†æœªè§è¿‡çš„ç‰¹å¾å€¼ï¼ˆä½¿ç”¨å¾ˆå°çš„æ¦‚ç‡ï¼‰
                    log_prob += np.log(1e-10)
            
            class_scores[class_label] = log_prob
        
        # å½’ä¸€åŒ–ä¸ºæ¦‚ç‡
        max_score = max(class_scores.values())
        normalized_scores = {}
        total = 0
        
        for class_label, score in class_scores.items():
            prob = np.exp(score - max_score)
            normalized_scores[class_label] = prob
            total += prob
        
        for class_label in normalized_scores:
            normalized_scores[class_label] /= total
        
        # è¿”å›æœ€å¯èƒ½çš„ç±»åˆ«
        best_class = max(normalized_scores, key=normalized_scores.get)
        return best_class, normalized_scores

def demo_bayesian_network():
    """æ¼”ç¤ºè´å¶æ–¯ç½‘ç»œ"""
    print("\n" + "="*50)
    print("è´å¶æ–¯ç½‘ç»œæ¼”ç¤º")
    print("="*50)
    
    # åˆ›å»ºç®€å•çš„åŒ»ç–—è¯Šæ–­ç½‘ç»œ
    bn = BayesianNetwork()
    
    print("\næ„å»ºåŒ»ç–—è¯Šæ–­è´å¶æ–¯ç½‘ç»œ:")
    print("å˜é‡: æ„Ÿå†’(Cold), å‘çƒ§(Fever), å¤´ç—›(Headache)")
    
    # æ·»åŠ å˜é‡
    bn.add_variable("Cold", [True, False])
    bn.add_variable("Fever", [True, False], ["Cold"])
    bn.add_variable("Headache", [True, False], ["Cold"])
    
    # è®¾ç½®å…ˆéªŒæ¦‚ç‡ P(Cold)
    bn.set_probability("Cold", {}, True, 0.1)   # æ„Ÿå†’æ¦‚ç‡10%
    bn.set_probability("Cold", {}, False, 0.9)  # ä¸æ„Ÿå†’æ¦‚ç‡90%
    
    # è®¾ç½®æ¡ä»¶æ¦‚ç‡ P(Fever|Cold)
    bn.set_probability("Fever", {"Cold": True}, True, 0.8)   # æ„Ÿå†’æ—¶å‘çƒ§æ¦‚ç‡80%
    bn.set_probability("Fever", {"Cold": True}, False, 0.2)  # æ„Ÿå†’æ—¶ä¸å‘çƒ§æ¦‚ç‡20%
    bn.set_probability("Fever", {"Cold": False}, True, 0.1)  # ä¸æ„Ÿå†’æ—¶å‘çƒ§æ¦‚ç‡10%
    bn.set_probability("Fever", {"Cold": False}, False, 0.9) # ä¸æ„Ÿå†’æ—¶ä¸å‘çƒ§æ¦‚ç‡90%
    
    # è®¾ç½®æ¡ä»¶æ¦‚ç‡ P(Headache|Cold)
    bn.set_probability("Headache", {"Cold": True}, True, 0.7)   # æ„Ÿå†’æ—¶å¤´ç—›æ¦‚ç‡70%
    bn.set_probability("Headache", {"Cold": True}, False, 0.3)  # æ„Ÿå†’æ—¶ä¸å¤´ç—›æ¦‚ç‡30%
    bn.set_probability("Headache", {"Cold": False}, True, 0.2)  # ä¸æ„Ÿå†’æ—¶å¤´ç—›æ¦‚ç‡20%
    bn.set_probability("Headache", {"Cold": False}, False, 0.8) # ä¸æ„Ÿå†’æ—¶ä¸å¤´ç—›æ¦‚ç‡80%
    
    # æŸ¥è¯¢ï¼šæ²¡æœ‰ä»»ä½•ç—‡çŠ¶æ—¶æ„Ÿå†’çš„æ¦‚ç‡
    print("\næŸ¥è¯¢1: P(Cold) - å…ˆéªŒæ¦‚ç‡")
    cold_prior = bn.query("Cold")
    for value, prob in cold_prior.items():
        print(f"  P(Cold={value}) = {prob:.3f}")
    
    # æŸ¥è¯¢ï¼šå‘çƒ§æ—¶æ„Ÿå†’çš„æ¦‚ç‡
    print("\næŸ¥è¯¢2: P(Cold|Fever=True) - å‘çƒ§æ—¶æ„Ÿå†’æ¦‚ç‡")
    cold_given_fever = bn.query("Cold", {"Fever": True})
    for value, prob in cold_given_fever.items():
        print(f"  P(Cold={value}|Fever=True) = {prob:.3f}")
    
    # æŸ¥è¯¢ï¼šå‘çƒ§ä¸”å¤´ç—›æ—¶æ„Ÿå†’çš„æ¦‚ç‡
    print("\næŸ¥è¯¢3: P(Cold|Fever=True, Headache=True) - å‘çƒ§ä¸”å¤´ç—›æ—¶æ„Ÿå†’æ¦‚ç‡")
    cold_given_symptoms = bn.query("Cold", {"Fever": True, "Headache": True})
    for value, prob in cold_given_symptoms.items():
        print(f"  P(Cold={value}|Fever=True, Headache=True) = {prob:.3f}")

def demo_markov_chain():
    """æ¼”ç¤ºé©¬å°”å¯å¤«é“¾"""
    print("\n" + "="*50)
    print("é©¬å°”å¯å¤«é“¾æ¼”ç¤º")
    print("="*50)
    
    # åˆ›å»ºå¤©æ°”é©¬å°”å¯å¤«é“¾
    weather_states = ["æ™´å¤©", "é›¨å¤©", "é˜´å¤©"]
    mc = MarkovChain(weather_states)
    
    print("\næ„å»ºå¤©æ°”é©¬å°”å¯å¤«é“¾:")
    
    # è®¾ç½®è½¬ç§»æ¦‚ç‡
    # ä»æ™´å¤©è½¬ç§»
    mc.set_transition_probability("æ™´å¤©", "æ™´å¤©", 0.7)
    mc.set_transition_probability("æ™´å¤©", "é˜´å¤©", 0.2)
    mc.set_transition_probability("æ™´å¤©", "é›¨å¤©", 0.1)
    
    # ä»é˜´å¤©è½¬ç§»
    mc.set_transition_probability("é˜´å¤©", "æ™´å¤©", 0.3)
    mc.set_transition_probability("é˜´å¤©", "é˜´å¤©", 0.4)
    mc.set_transition_probability("é˜´å¤©", "é›¨å¤©", 0.3)
    
    # ä»é›¨å¤©è½¬ç§»
    mc.set_transition_probability("é›¨å¤©", "æ™´å¤©", 0.2)
    mc.set_transition_probability("é›¨å¤©", "é˜´å¤©", 0.6)
    mc.set_transition_probability("é›¨å¤©", "é›¨å¤©", 0.2)
    
    # è®¾ç½®åˆå§‹åˆ†å¸ƒ
    mc.set_initial_probability("æ™´å¤©", 0.6)
    mc.set_initial_probability("é˜´å¤©", 0.3)
    mc.set_initial_probability("é›¨å¤©", 0.1)
    
    print("è½¬ç§»çŸ©é˜µ:")
    print(f"{'':>6} {'æ™´å¤©':>6} {'é˜´å¤©':>6} {'é›¨å¤©':>6}")
    for i, from_state in enumerate(weather_states):
        row = f"{from_state:>6}"
        for j, to_state in enumerate(weather_states):
            row += f" {mc.transition_matrix[i, j]:>6.2f}"
        print(row)
    
    # è®¡ç®—ç¨³æ€åˆ†å¸ƒ
    stationary = mc.get_stationary_distribution()
    print(f"\nç¨³æ€åˆ†å¸ƒ:")
    for state, prob in stationary.items():
        print(f"  P({state}) = {prob:.3f}")
    
    # æ¨¡æ‹Ÿå¤©æ°”åºåˆ—
    print(f"\næ¨¡æ‹Ÿ10å¤©å¤©æ°”:")
    sequence = mc.simulate(10, "æ™´å¤©")
    print(f"  {' -> '.join(sequence)}")

def demo_naive_bayes():
    """æ¼”ç¤ºæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨"""
    print("\n" + "="*50)
    print("æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨æ¼”ç¤º")
    print("="*50)
    
    # åˆ›å»ºæ–‡æœ¬åˆ†ç±»æ•°æ®
    training_data = [
        ({"å¥½": True, "ç”µå½±": True, "å–œæ¬¢": True}, "æ­£é¢"),
        ({"å¥½": True, "ç”µå½±": True, "æ¨è": True}, "æ­£é¢"),
        ({"ä¸é”™": True, "å€¼å¾—": True, "è§‚çœ‹": True}, "æ­£é¢"),
        ({"ç²¾å½©": True, "æ¼”æŠ€": True, "å¾ˆæ£’": True}, "æ­£é¢"),
        ({"ç³Ÿç³•": True, "ç”µå½±": True, "å¤±æœ›": True}, "è´Ÿé¢"),
        ({"æ— èŠ": True, "å‰§æƒ…": True, "å·®": True}, "è´Ÿé¢"),
        ({"æµªè´¹": True, "æ—¶é—´": True, "ä¸å¥½": True}, "è´Ÿé¢"),
        ({"çƒ‚": True, "ç‰‡": True, "ä¸æ¨è": True}, "è´Ÿé¢"),
    ]
    
    print("\nè®­ç»ƒæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨:")
    print("è®­ç»ƒæ•°æ®ï¼ˆç”µå½±è¯„è®ºåˆ†ç±»ï¼‰:")
    for features, label in training_data:
        feature_str = ", ".join(f for f in features.keys())
        print(f"  [{feature_str}] -> {label}")
    
    # è®­ç»ƒåˆ†ç±»å™¨
    nb = NaiveBayesClassifier()
    nb.train(training_data)
    
    print(f"\nå­¦ä¹ åˆ°çš„ç±»åˆ«æ¦‚ç‡:")
    for class_label, prob in nb.class_probabilities.items():
        print(f"  P({class_label}) = {prob:.3f}")
    
    # æµ‹è¯•åˆ†ç±»
    test_cases = [
        {"å¥½": True, "æ¨è": True},
        {"ç³Ÿç³•": True, "æ— èŠ": True},
        {"ä¸é”™": True, "ç”µå½±": True},
        {"çƒ‚": True, "ä¸å¥½": True}
    ]
    
    print(f"\nåˆ†ç±»æµ‹è¯•:")
    for test_features in test_cases:
        predicted_class, probabilities = nb.classify(test_features)
        feature_str = ", ".join(test_features.keys())
        print(f"  [{feature_str}] -> {predicted_class}")
        for class_label, prob in probabilities.items():
            print(f"    P({class_label}) = {prob:.3f}")

def demo_uncertainty_reasoning():
    """æ¼”ç¤ºä¸ç¡®å®šæ€§æ¨ç†"""
    print("\n" + "="*50)
    print("ä¸ç¡®å®šæ€§æ¨ç†æ¼”ç¤º")
    print("="*50)
    
    print("\nè´å¶æ–¯å®šç†åº”ç”¨ - åŒ»ç–—è¯Šæ–­:")
    print("è®¾ç½®:")
    print("  - ç–¾ç—…æ‚£ç—…ç‡: P(Disease) = 0.01")
    print("  - æ£€æµ‹æ•æ„Ÿåº¦: P(+|Disease) = 0.95")
    print("  - æ£€æµ‹ç‰¹å¼‚åº¦: P(-|Â¬Disease) = 0.98")
    
    # ä½¿ç”¨è´å¶æ–¯å®šç†è®¡ç®—
    p_disease = 0.01
    p_no_disease = 0.99
    p_positive_given_disease = 0.95
    p_negative_given_no_disease = 0.98
    p_positive_given_no_disease = 0.02
    
    # P(+) = P(+|Disease)P(Disease) + P(+|Â¬Disease)P(Â¬Disease)
    p_positive = (p_positive_given_disease * p_disease + 
                 p_positive_given_no_disease * p_no_disease)
    
    # P(Disease|+) = P(+|Disease)P(Disease) / P(+)
    p_disease_given_positive = (p_positive_given_disease * p_disease) / p_positive
    
    print(f"\nè®¡ç®—ç»“æœ:")
    print(f"  P(æ£€æµ‹é˜³æ€§) = {p_positive:.4f}")
    print(f"  P(æ‚£ç—…|æ£€æµ‹é˜³æ€§) = {p_disease_given_positive:.4f}")
    print(f"  è§£é‡Š: å³ä½¿æ£€æµ‹é˜³æ€§ï¼Œæ‚£ç—…æ¦‚ç‡ä»ç„¶è¾ƒä½ ({p_disease_given_positive*100:.1f}%)")

def visualize_probability_distributions():
    """å¯è§†åŒ–æ¦‚ç‡åˆ†å¸ƒ"""
    print("\n" + "="*50)
    print("æ¦‚ç‡åˆ†å¸ƒå¯è§†åŒ–")
    print("="*50)
    
    # åˆ›å»ºå›¾å½¢
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    fig.suptitle('æ¦‚ç‡åˆ†å¸ƒç¤ºä¾‹', fontsize=16)
    
    # 1. äºŒé¡¹åˆ†å¸ƒ
    n, p = 20, 0.3
    x = np.arange(0, n+1)
    y = [math.comb(n, k) * (p**k) * ((1-p)**(n-k)) for k in x]
    axes[0, 0].bar(x, y, alpha=0.7)
    axes[0, 0].set_title(f'äºŒé¡¹åˆ†å¸ƒ (n={n}, p={p})')
    axes[0, 0].set_xlabel('æˆåŠŸæ¬¡æ•°')
    axes[0, 0].set_ylabel('æ¦‚ç‡')
    
    # 2. æ­£æ€åˆ†å¸ƒ
    x = np.linspace(-4, 4, 100)
    y = (1/np.sqrt(2*np.pi)) * np.exp(-0.5 * x**2)
    axes[0, 1].plot(x, y, 'b-', linewidth=2)
    axes[0, 1].set_title('æ ‡å‡†æ­£æ€åˆ†å¸ƒ')
    axes[0, 1].set_xlabel('å€¼')
    axes[0, 1].set_ylabel('æ¦‚ç‡å¯†åº¦')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. æŒ‡æ•°åˆ†å¸ƒ
    x = np.linspace(0, 5, 100)
    lambda_param = 1.5
    y = lambda_param * np.exp(-lambda_param * x)
    axes[1, 0].plot(x, y, 'r-', linewidth=2)
    axes[1, 0].set_title(f'æŒ‡æ•°åˆ†å¸ƒ (Î»={lambda_param})')
    axes[1, 0].set_xlabel('å€¼')
    axes[1, 0].set_ylabel('æ¦‚ç‡å¯†åº¦')
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. å‡åŒ€åˆ†å¸ƒ
    x = np.array([0, 0, 2, 2, 4, 4])
    y = np.array([0, 0.5, 0.5, 0.5, 0.5, 0])
    axes[1, 1].plot(x, y, 'g-', linewidth=2)
    axes[1, 1].fill_between([0, 4], [0.5, 0.5], alpha=0.3)
    axes[1, 1].set_title('å‡åŒ€åˆ†å¸ƒ [0, 4]')
    axes[1, 1].set_xlabel('å€¼')
    axes[1, 1].set_ylabel('æ¦‚ç‡å¯†åº¦')
    axes[1, 1].set_ylim(0, 0.6)
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('probability_distributions.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("æ¦‚ç‡åˆ†å¸ƒå›¾å·²ä¿å­˜ä¸º 'probability_distributions.png'")

def run_comprehensive_demo():
    """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
    print("ğŸ² ç¬¬13ç« ï¼šæ¦‚ç‡æ¨ç† - å®Œæ•´æ¼”ç¤º")
    print("="*60)
    
    # è¿è¡Œå„ä¸ªæ¼”ç¤º
    demo_bayesian_network()
    demo_markov_chain()
    demo_naive_bayes()
    demo_uncertainty_reasoning()
    visualize_probability_distributions()
    
    print("\n" + "="*60)
    print("æ¦‚ç‡æ¨ç†æ¼”ç¤ºå®Œæˆï¼")
    print("="*60)
    print("\nğŸ“š å­¦ä¹ è¦ç‚¹:")
    print("â€¢ è´å¶æ–¯ç½‘ç»œæä¾›äº†æ¦‚ç‡ä¾èµ–å…³ç³»çš„å›¾å½¢è¡¨ç¤º")
    print("â€¢ é©¬å°”å¯å¤«é“¾å»ºæ¨¡çŠ¶æ€åºåˆ—çš„æ¦‚ç‡æ¼”åŒ–")
    print("â€¢ æœ´ç´ è´å¶æ–¯æ˜¯ç®€å•è€Œæœ‰æ•ˆçš„æ¦‚ç‡åˆ†ç±»æ–¹æ³•")
    print("â€¢ è´å¶æ–¯å®šç†æ˜¯ä¸ç¡®å®šæ€§æ¨ç†çš„æ ¸å¿ƒ")
    print("â€¢ æ¦‚ç‡æ¨ç†å¸®åŠ©åœ¨ä¸ç¡®å®šç¯å¢ƒä¸­åšå‡ºç†æ€§å†³ç­–")

if __name__ == "__main__":
    run_comprehensive_demo() 